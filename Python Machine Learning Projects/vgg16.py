# -*- coding: utf-8 -*-
"""VGG16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18U8fsbZgOVzJ4pXT0bfft0at2CjMnXKA
"""

import glob
import os, sys
import random
from tqdm import tqdm

import numpy as np
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from keras.models import Sequential
from keras.layers import Dropout, Flatten, Dense
from keras import applications

import seaborn as sns
import pandas as pd

import matplotlib.pyplot as plt

from keras import backend as K
K.tensorflow_backend._get_available_gpus()

! pip install -q kaggle

from google.colab import files
files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c dogs-vs-cats-redux-kernels-edition

import zipfile
import os, shutil
import pandas as pd
from PIL import Image
import random


for file in os.listdir():
 if file.endswith('.zip'):
  zip_ref = zipfile.ZipFile(file, 'r')
  zip_ref.extractall()
  zip_ref.close()

root_path = '/content'
Image.open(root_path+'/train/'+'cat.0.jpg')

# Please correct the urls accordingly

# The path to the directory where the original
# dataset was uncompressed
original_dataset_dir = '/content/train/'

# The directory where we will
# store our smaller dataset
base_dir = '/content/data1'
os.mkdir(base_dir)

# Directories for our training,
# validation and test splits
train_dir = os.path.join(base_dir, 'train')
os.mkdir(train_dir)
validation_dir = os.path.join(base_dir, 'validation')
os.mkdir(validation_dir)
test_dir = os.path.join(base_dir, 'test')
os.mkdir(test_dir)

# Directory with our training cat pictures
train_cats_dir = os.path.join(train_dir, 'cats')
os.mkdir(train_cats_dir)

# Directory with our training dog pictures
train_dogs_dir = os.path.join(train_dir, 'dogs')
os.mkdir(train_dogs_dir)

# Directory with our validation cat pictures
validation_cats_dir = os.path.join(validation_dir, 'cats')
os.mkdir(validation_cats_dir)

# Directory with our validation dog pictures
validation_dogs_dir = os.path.join(validation_dir, 'dogs')
os.mkdir(validation_dogs_dir)

# Directory with our validation cat pictures
test_cats_dir = os.path.join(test_dir, 'cats')
os.mkdir(test_cats_dir)

# Directory with our validation dog pictures
test_dogs_dir = os.path.join(test_dir, 'dogs')
os.mkdir(test_dogs_dir)

# Copy first 100 cat images to train_cats_dir
fnames = ['cat.{}.jpg'.format(i) for i in range(100)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(train_cats_dir, fname)
    shutil.copyfile(src, dst)

# Copy next 50 cat images to validation_cats_dir
fnames = ['cat.{}.jpg'.format(i) for i in range(100, 150)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(validation_cats_dir, fname)
    shutil.copyfile(src, dst)

# Copy next 50 cat images to test_cats_dir
fnames = ['cat.{}.jpg'.format(i) for i in range(150, 200)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(test_cats_dir, fname)
    shutil.copyfile(src, dst)

# Copy first 100 dog images to train_dogs_dir
fnames = ['dog.{}.jpg'.format(i) for i in range(100)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(train_dogs_dir, fname)
    shutil.copyfile(src, dst)

# Copy next 50 dog images to validation_dogs_dir
fnames = ['dog.{}.jpg'.format(i) for i in range(100, 150)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(validation_dogs_dir, fname)
    shutil.copyfile(src, dst)

# Copy next 50 dog images to test_dogs_dir
fnames = ['dog.{}.jpg'.format(i) for i in range(150, 200)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(test_dogs_dir, fname)
    shutil.copyfile(src, dst)

"""**Image preprocessing**

"""

from keras.preprocessing.image import ImageDataGenerator

# All images will be rescaled by 1./255
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        # This is the target directory
        train_dir,
        # All images will be resized to 150x150
        target_size=(150, 150),
        batch_size=20,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=20,
        class_mode='categorical')

for data_batch, labels_batch in train_generator:
    print('data batch shape:', data_batch.shape)
    print('labels batch shape:', labels_batch.shape)
    break

from keras import layers
from keras import models

model = models.Sequential()
model.add(layers.Flatten(input_shape=(150, 150, 3)))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))



"""**VGG16 GENERATOR**"""

from keras.applications.vgg16 import VGG16
model = VGG16()

img_shape = (150, 150, 3)
vgg16_model=VGG16(input_shape=img_shape,include_top=False, weights='imagenet')

vgg16_model.trainable=False
global_average_layer = layers.GlobalAveragePooling2D()
prediction_layer = layers.Dense(2,activation='softmax')

model = models.Sequential([
  vgg16_model,
  global_average_layer,
  prediction_layer
])

from keras import optimizers

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),
              metrics=['acc'])

history = model.fit(train_generator,
                    epochs=4,
                    steps_per_epoch=20,
                    validation_steps=20,
                    validation_data=validation_generator)

from keras.preprocessing.image import load_img
image=load_img(root_path+'/train/'+'dog.91.jpg', target_size=(150,150))

from keras.preprocessing.image import img_to_array
image=img_to_array(image)

image=image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
from keras.applications.vgg16 import preprocess_input
image=preprocess_input(image)
yhat=model.predict(image)
print(yhat)



"""**RESNET-50**

"""

from keras.applications.resnet50 import ResNet50
from keras.preprocessing import image
from keras.applications.resnet50 import preprocess_input, decode_predictions

img_shape = (150, 150, 3)
resnet50_model=ResNet50(input_shape=img_shape,include_top=False, weights='imagenet')
resnet50_model.trainable=False
global_average_layer = layers.GlobalAveragePooling2D()
prediction_layer = layers.Dense(2,activation='softmax')

model = models.Sequential([
  resnet50_model,
  global_average_layer,
  prediction_layer
])

from keras import optimizers

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),
              metrics=['acc'])

history = model.fit(train_generator,
                    epochs=4,
                    steps_per_epoch=20,
                    validation_steps=20,
                    validation_data=validation_generator)

from keras.preprocessing.image import load_img
image=load_img(root_path+'/train/'+'cat.91.jpg', target_size=(150,150))

from keras.preprocessing.image import img_to_array
image=img_to_array(image)

image=image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
from keras.applications.vgg16 import preprocess_input
image=preprocess_input(image)
yhat=model.predict(image)
print(yhat)

"""**GoogleLeNet**"""

pip install pillow numpy imageio

from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.applications.inception_v3 import preprocess_input, decode_predictions

img_shape = (150, 150, 3)
InceptionV3_model=InceptionV3(input_shape=img_shape,include_top=False, weights='imagenet')
InceptionV3_model.trainable=False
global_average_layer = layers.GlobalAveragePooling2D()
prediction_layer = layers.Dense(2,activation='softmax')

model = models.Sequential([
  InceptionV3_model,
  global_average_layer,
  prediction_layer
])

from keras import optimizers

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),
              metrics=['acc'])

history = model.fit(train_generator,
                    epochs=4,
                    steps_per_epoch=20,
                    validation_steps=20,
                    validation_data=validation_generator)

from keras.preprocessing.image import load_img
image=load_img(root_path+'/train/'+'cat.987.jpg', target_size=(150,150))

from keras.preprocessing.image import img_to_array
image=img_to_array(image)

image=image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
from keras.applications.vgg16 import preprocess_input
image=preprocess_input(image)
yhat=model.predict(image)
print(yhat)